{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from time import time\n",
    "import math\n",
    "from torch.nn import init\n",
    "import copy\n",
    "import cv2\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Configuration des paramètres\n",
    "config = {\n",
    "    \"epoch_num\": 200,\n",
    "    \"layer_num\": 9,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"group_num\": 1,\n",
    "    \"cs_ratio\": 1,\n",
    "    \"gpu_list\": \"0\",\n",
    "    \"matrix_dir\": \"sampling_matrix\",\n",
    "    \"model_dir\": \"model\",\n",
    "    \"data_dir\": \"data\",\n",
    "    \"log_dir\": \"log\",\n",
    "    \"result_dir\": \"result\",\n",
    "    \"test_name\": \"Set11\"\n",
    "}\n",
    "\n",
    "# Configuration GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config[\"gpu_list\"]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ratio_dict = {1: 10, 4: 43, 10: 109, 25: 272, 30: 327, 40: 436, 50: 545}\n",
    "n_input = ratio_dict[config[\"cs_ratio\"]]\n",
    "n_output = 1089\n",
    "\n",
    "# Chargement de la matrice de sampling\n",
    "Phi_data_Name = f'./{config[\"matrix_dir\"]}/phi_0_{config[\"cs_ratio\"]}_1089.mat'\n",
    "Phi_data = sio.loadmat(Phi_data_Name)\n",
    "Phi_input = Phi_data['phi']\n",
    "\n",
    "# Définition du modèle ISTA-Net\n",
    "class BasicBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.lambda_step = nn.Parameter(torch.Tensor([0.5]))\n",
    "        self.soft_thr = nn.Parameter(torch.Tensor([0.01]))\n",
    "        self.conv1_forward = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 1, 3, 3)))\n",
    "        self.conv2_forward = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3)))\n",
    "        self.conv1_backward = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3)))\n",
    "        self.conv2_backward = nn.Parameter(init.xavier_normal_(torch.Tensor(1, 32, 3, 3)))\n",
    "    \n",
    "    def forward(self, x, PhiTPhi, PhiTb):\n",
    "        x = x - self.lambda_step * torch.mm(x, PhiTPhi)\n",
    "        x = x + self.lambda_step * PhiTb\n",
    "        x_input = x.view(-1, 1, 33, 33)\n",
    "        x = F.relu(F.conv2d(x_input, self.conv1_forward, padding=1))\n",
    "        x_forward = F.conv2d(x, self.conv2_forward, padding=1)\n",
    "        x = torch.mul(torch.sign(x_forward), F.relu(torch.abs(x_forward) - self.soft_thr))\n",
    "        x = F.relu(F.conv2d(x, self.conv1_backward, padding=1))\n",
    "        x_backward = F.conv2d(x, self.conv2_backward, padding=1)\n",
    "        x_pred = x_backward.view(-1, 1089)\n",
    "        return x_pred\n",
    "\n",
    "class ISTANet(torch.nn.Module):\n",
    "    def __init__(self, LayerNo):\n",
    "        super(ISTANet, self).__init__()\n",
    "        self.LayerNo = LayerNo\n",
    "        self.fcs = nn.ModuleList([BasicBlock() for _ in range(LayerNo)])\n",
    "    \n",
    "    def forward(self, Phix, Phi, Qinit):\n",
    "        PhiTPhi = torch.mm(Phi.T, Phi)\n",
    "        PhiTb = torch.mm(Phix, Phi)\n",
    "        x = torch.mm(Phix, Qinit.T)\n",
    "        for layer in self.fcs:\n",
    "            x = layer(x, PhiTPhi, PhiTb)\n",
    "        return x\n",
    "\n",
    "# Chargement du modèle\n",
    "model = ISTANet(config[\"layer_num\"])\n",
    "model = nn.DataParallel(model).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "model_dir = f\"./{config['model_dir']}/CS_ISTA_Net_layer_{config['layer_num']}_group_{config['group_num']}_ratio_{config['cs_ratio']}_lr_{config['learning_rate']:.4f}\"\n",
    "model.load_state_dict(torch.load(f'{model_dir}/net_params_{config[\"epoch_num\"]}.pkl', map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: module.fcs.0.lambda_step, Shape: torch.Size([1])\n",
      "Layer: module.fcs.0.soft_thr, Shape: torch.Size([1])\n",
      "Layer: module.fcs.0.conv1_forward, Shape: torch.Size([32, 1, 3, 3])\n",
      "Layer: module.fcs.0.conv2_forward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.0.conv1_backward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.0.conv2_backward, Shape: torch.Size([1, 32, 3, 3])\n",
      "Layer: module.fcs.1.lambda_step, Shape: torch.Size([1])\n",
      "Layer: module.fcs.1.soft_thr, Shape: torch.Size([1])\n",
      "Layer: module.fcs.1.conv1_forward, Shape: torch.Size([32, 1, 3, 3])\n",
      "Layer: module.fcs.1.conv2_forward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.1.conv1_backward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.1.conv2_backward, Shape: torch.Size([1, 32, 3, 3])\n",
      "Layer: module.fcs.2.lambda_step, Shape: torch.Size([1])\n",
      "Layer: module.fcs.2.soft_thr, Shape: torch.Size([1])\n",
      "Layer: module.fcs.2.conv1_forward, Shape: torch.Size([32, 1, 3, 3])\n",
      "Layer: module.fcs.2.conv2_forward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.2.conv1_backward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.2.conv2_backward, Shape: torch.Size([1, 32, 3, 3])\n",
      "Layer: module.fcs.3.lambda_step, Shape: torch.Size([1])\n",
      "Layer: module.fcs.3.soft_thr, Shape: torch.Size([1])\n",
      "Layer: module.fcs.3.conv1_forward, Shape: torch.Size([32, 1, 3, 3])\n",
      "Layer: module.fcs.3.conv2_forward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.3.conv1_backward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.3.conv2_backward, Shape: torch.Size([1, 32, 3, 3])\n",
      "Layer: module.fcs.4.lambda_step, Shape: torch.Size([1])\n",
      "Layer: module.fcs.4.soft_thr, Shape: torch.Size([1])\n",
      "Layer: module.fcs.4.conv1_forward, Shape: torch.Size([32, 1, 3, 3])\n",
      "Layer: module.fcs.4.conv2_forward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.4.conv1_backward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.4.conv2_backward, Shape: torch.Size([1, 32, 3, 3])\n",
      "Layer: module.fcs.5.lambda_step, Shape: torch.Size([1])\n",
      "Layer: module.fcs.5.soft_thr, Shape: torch.Size([1])\n",
      "Layer: module.fcs.5.conv1_forward, Shape: torch.Size([32, 1, 3, 3])\n",
      "Layer: module.fcs.5.conv2_forward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.5.conv1_backward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.5.conv2_backward, Shape: torch.Size([1, 32, 3, 3])\n",
      "Layer: module.fcs.6.lambda_step, Shape: torch.Size([1])\n",
      "Layer: module.fcs.6.soft_thr, Shape: torch.Size([1])\n",
      "Layer: module.fcs.6.conv1_forward, Shape: torch.Size([32, 1, 3, 3])\n",
      "Layer: module.fcs.6.conv2_forward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.6.conv1_backward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.6.conv2_backward, Shape: torch.Size([1, 32, 3, 3])\n",
      "Layer: module.fcs.7.lambda_step, Shape: torch.Size([1])\n",
      "Layer: module.fcs.7.soft_thr, Shape: torch.Size([1])\n",
      "Layer: module.fcs.7.conv1_forward, Shape: torch.Size([32, 1, 3, 3])\n",
      "Layer: module.fcs.7.conv2_forward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.7.conv1_backward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.7.conv2_backward, Shape: torch.Size([1, 32, 3, 3])\n",
      "Layer: module.fcs.8.lambda_step, Shape: torch.Size([1])\n",
      "Layer: module.fcs.8.soft_thr, Shape: torch.Size([1])\n",
      "Layer: module.fcs.8.conv1_forward, Shape: torch.Size([32, 1, 3, 3])\n",
      "Layer: module.fcs.8.conv2_forward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.8.conv1_backward, Shape: torch.Size([32, 32, 3, 3])\n",
      "Layer: module.fcs.8.conv2_backward, Shape: torch.Size([1, 32, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}, Shape: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "weights = model.state_dict()[\"module.fcs.8.conv2_backward\"]\n",
    "print(weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(2.4472)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.state_dict()['module.fcs.8.lambda_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poids de la dernière couche: tensor([[[[ 9.3944e-02,  7.8764e-03,  4.3230e-02],\n",
      "          [ 1.5584e-01,  4.0211e-02,  5.5612e-02],\n",
      "          [-2.0164e-02,  4.6392e-02, -3.4072e-02]],\n",
      "\n",
      "         [[ 1.0583e-01, -1.8420e-02,  5.8330e-02],\n",
      "          [ 4.7325e-02,  1.6555e-02,  5.8020e-02],\n",
      "          [-3.3384e-02,  1.6411e-01, -2.1265e-03]],\n",
      "\n",
      "         [[-3.6796e-02,  6.3130e-02,  8.4416e-02],\n",
      "          [-9.0057e-03,  6.1645e-02,  7.4977e-02],\n",
      "          [-3.6470e-02,  4.1247e-02,  8.5854e-02]],\n",
      "\n",
      "         [[-4.0988e-02, -4.5996e-02, -3.7543e-02],\n",
      "          [-3.3075e-02, -1.4151e-02, -1.7834e-02],\n",
      "          [-6.7438e-02, -5.0019e-02, -5.9243e-02]],\n",
      "\n",
      "         [[ 1.8537e-01,  4.1767e-02, -2.4961e-02],\n",
      "          [ 1.4437e-01, -1.0833e-01,  1.2532e-01],\n",
      "          [ 1.5701e-01,  2.0168e-01,  2.3425e-01]],\n",
      "\n",
      "         [[ 6.9351e-01,  2.6046e-01,  4.4288e-02],\n",
      "          [ 1.8627e-01,  3.5107e-01,  3.7830e-02],\n",
      "          [ 1.2682e-01,  6.9190e-02,  3.4096e-02]],\n",
      "\n",
      "         [[ 3.0768e-02,  5.9904e-02,  2.1782e-02],\n",
      "          [ 5.9932e-02,  5.1531e-02, -2.6520e-02],\n",
      "          [ 3.1190e-02, -7.6017e-04, -9.9714e-03]],\n",
      "\n",
      "         [[-2.9756e-02, -1.6289e-02,  6.1553e-03],\n",
      "          [ 2.5677e-03,  3.5728e-02, -7.4795e-02],\n",
      "          [ 9.4730e-02, -4.2698e-02,  2.5549e-02]],\n",
      "\n",
      "         [[ 2.8878e-03, -2.6714e-02, -8.9214e-02],\n",
      "          [ 1.0204e-01,  9.1497e-02,  8.1790e-02],\n",
      "          [ 2.5286e-02,  1.0097e-01, -8.4816e-02]],\n",
      "\n",
      "         [[ 6.8469e-02,  1.7484e-01,  1.0958e-01],\n",
      "          [ 8.2944e-02,  1.5027e-01, -8.1077e-03],\n",
      "          [ 7.8732e-02,  2.4547e-01,  2.0557e-01]],\n",
      "\n",
      "         [[ 1.2491e-01,  2.1066e-01,  5.4507e-01],\n",
      "          [ 1.0138e-01,  2.1097e-01,  2.8483e-01],\n",
      "          [ 3.1623e-02,  2.2514e-04,  5.7448e-02]],\n",
      "\n",
      "         [[-1.1618e-01, -1.0102e-01, -8.7958e-02],\n",
      "          [-3.8068e-02, -5.8394e-02, -6.0405e-02],\n",
      "          [-7.6795e-02, -6.2354e-02, -9.5044e-02]],\n",
      "\n",
      "         [[-1.9044e-02,  3.5726e-02, -6.5302e-02],\n",
      "          [-1.8953e-02,  1.1992e-01,  9.7998e-02],\n",
      "          [ 7.9484e-02,  1.3644e-01, -1.5387e-03]],\n",
      "\n",
      "         [[ 3.5986e-02,  1.1516e-02,  1.1513e-02],\n",
      "          [ 5.7694e-02,  7.6678e-02,  1.1617e-01],\n",
      "          [ 1.0396e-01,  4.8976e-02,  5.5778e-02]],\n",
      "\n",
      "         [[-3.9631e-01, -3.9410e-01, -4.0815e-01],\n",
      "          [-3.4014e-01, -2.3161e-01, -3.1868e-01],\n",
      "          [-3.9335e-01, -3.7655e-01, -4.0687e-01]],\n",
      "\n",
      "         [[-3.5377e-02, -5.9228e-02, -8.0365e-03],\n",
      "          [ 1.6006e-01,  6.4886e-02,  4.0317e-02],\n",
      "          [ 5.4112e-02,  4.8440e-02,  7.3489e-02]],\n",
      "\n",
      "         [[-8.3515e-02, -7.3605e-02, -8.7206e-02],\n",
      "          [-5.5820e-02, -8.2539e-02, -1.3416e-01],\n",
      "          [-1.0137e-01, -8.5399e-02, -1.2557e-01]],\n",
      "\n",
      "         [[ 3.3559e-03,  4.5972e-02,  3.6902e-02],\n",
      "          [-2.0181e-02,  2.3445e-02, -2.0016e-03],\n",
      "          [ 6.3856e-02,  1.2246e-01,  9.7652e-02]],\n",
      "\n",
      "         [[-6.2829e-02, -6.3445e-02, -6.0240e-02],\n",
      "          [-4.7040e-02, -4.7253e-02, -6.3157e-02],\n",
      "          [-3.5256e-02, -4.6135e-02, -5.8526e-02]],\n",
      "\n",
      "         [[ 6.5643e-03,  4.4474e-02, -1.1365e-01],\n",
      "          [-4.8005e-02,  1.0654e-02,  7.2695e-02],\n",
      "          [ 9.0689e-02,  5.5621e-02, -3.8149e-03]],\n",
      "\n",
      "         [[-6.0125e-04, -3.9166e-02,  3.9614e-02],\n",
      "          [ 8.2569e-02, -9.5438e-02,  5.2847e-02],\n",
      "          [ 1.4277e-01,  6.2289e-02,  5.2146e-02]],\n",
      "\n",
      "         [[-2.6246e-02, -4.8725e-02, -5.3463e-02],\n",
      "          [-8.4427e-02, -2.3933e-02, -5.9086e-02],\n",
      "          [-7.5810e-02, -2.9770e-02, -9.1995e-02]],\n",
      "\n",
      "         [[ 2.1566e-01, -3.3375e-02,  4.9385e-02],\n",
      "          [ 3.1119e-03,  2.0470e-02,  9.9360e-03],\n",
      "          [ 2.4946e-02, -1.2283e-02, -1.3928e-02]],\n",
      "\n",
      "         [[ 2.6977e-03,  6.1327e-02,  9.7586e-02],\n",
      "          [ 1.1587e-02,  1.5175e-01,  3.5753e-02],\n",
      "          [ 1.2232e-01,  2.8294e-01,  4.0597e-01]],\n",
      "\n",
      "         [[ 1.5544e-01,  6.0299e-02,  6.8237e-03],\n",
      "          [ 5.4442e-02, -7.4778e-02, -2.3055e-02],\n",
      "          [ 8.5869e-03, -1.4510e-01,  1.0073e-02]],\n",
      "\n",
      "         [[-1.4547e-02,  1.2274e-01,  2.3349e-02],\n",
      "          [-3.0984e-02,  4.4480e-02, -9.5006e-02],\n",
      "          [-9.3548e-02,  4.1013e-02,  1.1045e-02]],\n",
      "\n",
      "         [[-3.8420e-02,  1.0161e-01, -3.2045e-02],\n",
      "          [-6.0805e-03,  1.9789e-01,  8.3319e-02],\n",
      "          [-5.0463e-02,  2.0535e-01, -2.1194e-02]],\n",
      "\n",
      "         [[-1.6384e-01, -2.6879e-01, -1.0505e-01],\n",
      "          [-1.7576e-01, -2.3314e-01, -1.1506e-01],\n",
      "          [-1.7612e-01, -2.2484e-01, -1.2025e-01]],\n",
      "\n",
      "         [[ 7.1959e-02,  1.2062e-01,  1.3588e-02],\n",
      "          [ 6.1005e-02, -5.7504e-02,  1.9727e-02],\n",
      "          [ 4.2173e-02, -1.5422e-04,  5.8324e-02]],\n",
      "\n",
      "         [[ 3.5742e-02,  6.6315e-02,  1.1984e-01],\n",
      "          [ 3.6203e-02, -1.9278e-01,  3.5671e-02],\n",
      "          [ 3.9674e-02,  6.7080e-02,  8.3233e-02]],\n",
      "\n",
      "         [[-1.0006e-01, -6.5151e-02, -4.1313e-02],\n",
      "          [-1.1861e-01,  1.3051e-01,  6.9028e-04],\n",
      "          [ 5.3923e-02,  3.6131e-02,  1.1925e-01]],\n",
      "\n",
      "         [[ 1.9085e-01,  5.6101e-02, -3.1303e-02],\n",
      "          [ 1.1408e-01,  9.0570e-03, -3.4923e-02],\n",
      "          [ 1.1691e-01, -3.9814e-02, -1.4925e-01]]]])\n"
     ]
    }
   ],
   "source": [
    "last_layer_name = list(model.state_dict().keys())[-1]\n",
    "last_layer_weights = model.state_dict()[last_layer_name]\n",
    "print(f'Poids de la dernière couche: {last_layer_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(2.4472)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.state_dict()[ 'module.fcs.8.lambda_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_fista = model.state_dict()['module.fcs.8.lambda_step'].item()\n",
    "soft_thr_fista = model.state_dict()['module.fcs.8.soft_thr'].item()\n",
    "\n",
    "conv1_f = model.state_dict()['module.fcs.8.conv1_forward'].detach().cpu().numpy()\n",
    "conv2_f = model.state_dict()['module.fcs.8.conv2_forward'].detach().cpu().numpy()\n",
    "conv1_b = model.state_dict()['module.fcs.8.conv1_backward'].detach().cpu().numpy()\n",
    "conv2_b = model.state_dict()['module.fcs.8.conv2_backward'].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.4471731185913086, -0.018895795568823814)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_fista, soft_thr_fista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "def soft_thresholding(x, threshold):\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "def convolve2d(x, kernel):\n",
    "    \"\"\" Convolution 2D avec padding symétrique \"\"\"\n",
    "    return scipy.signal.convolve2d(x, kernel, mode='same', boundary='symm')\n",
    "\n",
    "def fista_conv(y, Phi, PhiT, conv1_f, conv2_f, conv1_b, conv2_b, lambda_fista, soft_thr_fista, num_iters=50):\n",
    "    \"\"\"\n",
    "    Implémentation de FISTA en utilisant les convolutions apprises de ISTA-Net.\n",
    "    - y : observation (M, 1)\n",
    "    - Phi : matrice de mesure (M, N)\n",
    "    - PhiT : transposée de Phi (N, M)\n",
    "    \"\"\"\n",
    "\n",
    "    N = PhiT.shape[0]  # Taille de la reconstruction (doit être 1089 pour ISTA-Net)\n",
    "    H, W = 33, 33  # Dimensions d'image attendues (carré parfait pour convolution)\n",
    "\n",
    "    x = np.dot(PhiT, y)  # Estimation initiale (N, 1)\n",
    "    x = x.reshape(H, W)  # Reshape en image\n",
    "\n",
    "    t = 1\n",
    "    z = x.copy()\n",
    "\n",
    "    for k in range(num_iters):\n",
    "        gradient = np.dot(PhiT, np.dot(Phi, z.flatten())) - np.dot(PhiT, y).flatten()\n",
    "        x_new = soft_thresholding(z.flatten() - lambda_fista * gradient, lambda_fista)\n",
    "        x_new = x_new.reshape(H, W)  # Reshape en image 2D\n",
    "\n",
    "        # Appliquer les convolutions apprises\n",
    "        x_conv1 = convolve2d(x_new, conv1_f[0, 0])\n",
    "        x_relu1 = np.maximum(x_conv1, 0)\n",
    "        x_conv2 = convolve2d(x_relu1, conv2_f[0, 0])\n",
    "        \n",
    "        x_thresh = soft_thresholding(x_conv2, soft_thr_fista)\n",
    "\n",
    "        x_conv3 = convolve2d(x_thresh, conv1_b[0, 0])\n",
    "        x_relu2 = np.maximum(x_conv3, 0)\n",
    "        x_conv4 = convolve2d(x_relu2, conv2_b[0, 0])\n",
    "\n",
    "        x_final = x_conv4  # Image reconstruite\n",
    "\n",
    "        # Mise à jour de FISTA\n",
    "        t_new = (1 + np.sqrt(1 + 4 * t**2)) / 2\n",
    "        z = x_final + ((t - 1) / t_new) * (x_final - x)\n",
    "        x, t = x_final, t_new\n",
    "\n",
    "    return x_final  # Image (33x33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction terminée !\n",
      "Taille de l'image reconstruite : (33, 33)\n"
     ]
    }
   ],
   "source": [
    "# Générer une matrice Phi et un signal y avec la bonne taille\n",
    "N = 1089  # Taille de l'image aplatie (33x33)\n",
    "M = N // 2  # Moitié des mesures\n",
    "A = np.random.randn(M, N)  # Matrice de mesure (M x N)\n",
    "y = np.random.randn(M, 1)  # Signal observé (M x 1)\n",
    "A_T = A.T  # Transposée de Phi\n",
    "\n",
    "# Reconstruction avec FISTA utilisant les poids de ISTA-Net\n",
    "x_reconstructed = fista_conv(y, A, A_T, conv1_f, conv2_f, conv1_b, conv2_b, lambda_fista, soft_thr_fista)\n",
    "\n",
    "print(\"Reconstruction terminée !\")\n",
    "print(\"Taille de l'image reconstruite :\", x_reconstructed.shape)  # Doit être (33, 33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
